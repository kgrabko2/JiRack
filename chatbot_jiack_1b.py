# Copyright (c) 2025 CMS Manhattan
# All rights reserved.
# Author: Konstantin Vladimirovich Grabko
# Email: grabko@cmsmanhattan.com
# Phone: +1(516)777-0945
#
# This file is part of a project authored by CMS Manhattan. You may use, distribute, and modify
# this code under the terms of the GNU GENERAL PUBLIC LICENSE, Version 3, 29 June 2007.
# Please read <http://www.gnu.org/licenses/>.

# Fine tune JiRackPyTorch 1B ‚Äî final clean version, December 2025

import torch
import torch.nn.functional as F
from transformers import GPT2TokenizerFast
from JiRackPyTorch_GPT5_class_1b import JiRackPyTorch # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –∏–º–ø–æ—Ä—Ç, —á—Ç–æ –∏ –≤ fine_tune.py
import os
from pathlib import Path 

# ============================= –ù–ê–°–¢–†–û–ô–ö–ò –ì–ï–ù–ï–†–ê–¶–ò–ò =============================
# Temperature: –ß–µ–º –Ω–∏–∂–µ, —Ç–µ–º –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã –æ—Ç–≤–µ—Ç—ã.
# –ù–∞—á–Ω–∏—Ç–µ —Å 0.7. –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è, –ø–æ–≤—ã—Å—å—Ç–µ –¥–æ 0.8.
TEMPERATURE = 0.7 

# Top-K: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–±–æ—Ä–∫—É K –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏.
# –ù–∞—á–Ω–∏—Ç–µ —Å 50. –£–≤–µ–ª–∏—á–∏–≤–∞–π—Ç–µ, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç—ã —Å–ª–∏—à–∫–æ–º —Å–∫—É—á–Ω—ã–µ.
TOP_K = 50          

# Max Length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ —Ä–∞–∑
MAX_LENGTH = 120    

# ============================= –ü–£–¢–ò =============================
#LAST_TRAINED_PATH = Path("models/gpt_last_trained.pt")
LAST_TRAINED_PATH = Path("build/fine_tuning_output/epoch2/gpt_finetuned.pt")
#FINAL_OUTPUT_DIR = Path("build/fine_tuning_output/final")
FINAL_OUTPUT_DIR = Path("build/fine_tuning_output/epoch2/gpt_finetuned.pt")
MODEL_SAVE_NAME = "gpt_finetuned.pt"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ============================= –ö–õ–ê–°–° Chatbot =============================
class Chatbot:
    def __init__(self, model_path):
        # 1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        print("Loading standard tokenizer (gpt2)...")
        self.tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 2. –ú–æ–¥–µ–ª—å
        print("Initializing model...")
        self.model = JiRackPyTorch().to(device)
        self.model.eval()

        # –ü–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≤–µ—Å–æ–≤: —Å–Ω–∞—á–∞–ª–∞ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞, –ø–æ—Ç–æ–º last_trained
        load_path = None
        if (FINAL_OUTPUT_DIR / MODEL_SAVE_NAME).exists():
            load_path = FINAL_OUTPUT_DIR / MODEL_SAVE_NAME
            print(f"Weights for Epoch 50 found. Loading and moving to {device}...")
        elif model_path.exists():
            load_path = model_path
            print(f"Loading weights from {load_path} and moving to {device}...")
        
        if load_path:
            self.model.load_state_dict(torch.load(load_path, map_location=device))
        else:
            print("Warning: No trained weights found. Using initialized model.")
            
        print(f"Model successfully loaded on {device} and ready for chat!")

    def generate_response(self, prompt, max_length=MAX_LENGTH, temperature=TEMPERATURE, top_k=TOP_K):
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–≤–æ–¥
        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(device)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        with torch.no_grad():
            for _ in range(max_length):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
                logits, _ = self.model(input_ids)
                
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ª–æ–≥–∏—Ç—ã –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
                next_token_logits = logits[:, -1, :] 
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
                next_token_logits = next_token_logits / temperature
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º Top-K —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
                if top_k > 0:
                    # –û—Ç—Å–µ–∫–∞–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã, –∫—Ä–æ–º–µ TOP_K —Å–∞–º—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö
                    values, indices = torch.topk(next_token_logits, top_k)
                    # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                    next_token_logits = torch.full_like(next_token_logits, -float('inf'))
                    next_token_logits.scatter_(1, indices, values)

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ª–æ–≥–∏—Ç—ã –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏ —Å—ç–º–ø–ª–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
                probabilities = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probabilities, num_samples=1)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∫ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º
                input_ids = torch.cat([input_ids, next_token], dim=-1)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –¥–∏–∞–ª–æ–≥–∞ (__eou__) –∏–ª–∏ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞ (EOS)
                generated_token = self.tokenizer.decode(next_token.squeeze().item())
                if "__eou__" in generated_token or next_token.squeeze().item() == self.tokenizer.eos_token_id:
                    break

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –æ–±—Ä–µ–∑–∞—è –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        output = self.tokenizer.decode(input_ids.squeeze().tolist())
        
        # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º—Ç
        response = output[len(prompt):].strip()
        
        # –£–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –¥–∏–∞–ª–æ–≥–∞, –µ—Å–ª–∏ –æ–Ω –æ—Å—Ç–∞–ª—Å—è –≤ –∫–æ–Ω—Ü–µ
        response = response.replace("__eou__", "").strip()

        return response

def main():
    # === –ö–û–†–†–ï–ö–¢–ò–†–û–í–ö–ê –û–®–ò–ë–ö–ò: –û–±—ä—è–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
    global TEMPERATURE, TOP_K
    
    chatbot = Chatbot(LAST_TRAINED_PATH)

    print("\n" + "="*60)
    print(f"ü§ñ CHATBOT ACTIVATED (PPL 2.6 / Temperature {TEMPERATURE} / Top-K {TOP_K})")
    print("Enter 'exit' or 'quit' to quit. Use 'set temp=0.x' or 'set k=N' to change settings.")
    print("="*60 + "\n")

    while True:
        try:
            user_input = input(">>> You: ")
            if user_input.lower() in ['quit', 'exit']:
                break
            
            # –ö–æ–º–∞–Ω–¥—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if user_input.lower().startswith('set temp='):
                try:
                    # –¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–∏—Å–≤–∞–∏–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –æ–±—ä—è–≤–ª–µ–Ω—ã –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ –≤—ã—à–µ
                    TEMPERATURE = float(user_input.split('=')[1].strip())
                    print(f"ü§ñ Temperature set to {TEMPERATURE}")
                    continue
                except ValueError:
                    print("ü§ñ Invalid temperature value. Use 'set temp=0.x'.")
                    continue
            
            if user_input.lower().startswith('set k='):
                try:
                    # –¢–µ–ø–µ—Ä—å –º—ã –º–æ–∂–µ–º –ø—Ä–∏—Å–≤–∞–∏–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –æ–±—ä—è–≤–ª–µ–Ω—ã –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ –≤—ã—à–µ
                    TOP_K = int(user_input.split('=')[1].strip())
                    print(f"ü§ñ Top-K set to {TOP_K}")
                    continue
                except ValueError:
                    print("ü§ñ Invalid K value. Use 'set k=N' (e.g., set k=50).")
                    continue

            print("...Generating...")
            response = chatbot.generate_response(user_input)
            print(f"ü§ñ Model: {response}\n")

        except Exception as e:
            print(f"An error occurred: {e}")
            break

if __name__ == "__main__":
    from pathlib import Path
    main()